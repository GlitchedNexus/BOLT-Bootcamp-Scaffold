{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Preprocessing\n",
    "\n",
    "This notebook is for preprocessing the data. It will be used to clean the data and prepare it for training and exploratory data analysis.\n",
    "\n",
    "This ensures that the data is in a format that can be easily used for training and analysis. It also helps to remove any noise or irrelevant information from the data.\n",
    "\n",
    "The preprocessing steps may include:\n",
    "- Removing null values- Removing duplicates\n",
    "- Removing outliers\n",
    "- Normalizing the data\n",
    "- Encoding categorical variables\n",
    "- Feature engineering\n",
    "\n",
    "Depending on the specific dataset and the problem you are trying to solve, you may need to perform additional preprocessing steps or skip some steps entirely. The goal of this notebook is to give you a starting point for preprocessing your data, and you are encouraged to explore more advanced techniques on your own."
   ],
   "id": "c5c94ce7c9a70fcc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-22T23:23:12.983855Z",
     "start_time": "2026-02-22T23:23:12.981093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd"
   ],
   "id": "466cc2121c193007",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the raw data\n",
    "RAW_DATA_PATH = \"../data/raw/raw_data.csv\"\n",
    "data = pd.read_csv(RAW_DATA_PATH)"
   ],
   "id": "51ef6caa63a2899f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# See the first few rows of the data\n",
    "data.head()"
   ],
   "id": "53145f2c97159907"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check for common noise in the data such as null values and duplicates\n",
    "print(f\"Number of null values: {data.isnull().sum().sum()}\\n Number of duplicates: {data.duplicated().sum()}\")"
   ],
   "id": "3796616945666c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Remove null values and duplicates\n",
    "data = data.dropna()\n",
    "data = data.drop_duplicates()"
   ],
   "id": "885f49099dfc3017"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Check for outliers in the data using box plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.boxplot(data=data)\n",
    "plt.show()"
   ],
   "id": "d66a03ad34bf6fea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Remove outliers using the IQR method\n",
    "Q1 = data.quantile(0.25)\n",
    "Q3 = data.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "data = data[~((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))).any(axis=1)]"
   ],
   "id": "729f3419bdfd8816"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Normalize the data using min-max scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "data_scaled = pd.DataFrame(data_scaled, columns=data.columns)"
   ],
   "id": "5a9594f788f0ea6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Encode categorical variables using one-hot encoding\n",
    "data_encoded = pd.get_dummies(data_scaled, columns=[\"categorical_feature1\", \"categorical_feature2\"])"
   ],
   "id": "d596e796e72bcaa4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Rename the columns to remove any spaces or special characters\n",
    "data_encoded.columns = data_encoded.columns.str.replace(\" \", \"_\").str.replace(\"-\", \"_\")"
   ],
   "id": "3b8135d3286e661b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save the processed data\n",
    "PROCESSED_DATA_PATH = \"../data/processed/processed_data.csv\"\n",
    "data_encoded.to_csv(PROCESSED_DATA_PATH, index=False)"
   ],
   "id": "358e162d742edbc1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

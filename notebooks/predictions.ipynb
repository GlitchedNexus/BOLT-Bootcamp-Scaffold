{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Predictions\n",
    "\n",
    "In this notebook you can fit a model to the data and make predictions. Since we want you to figure things out on your own, we will only introduce some elementary models and techniques. You are encouraged to explore more advanced models and techniques on your own.\n",
    "\n",
    "You should always split your data into training and testing sets before fitting a model. This ensures that you can evaluate the performance of your model on unseen data. You can use the `train_test_split` function from the `sklearn.model_selection` module to split your data.\n",
    "\n",
    "The models we will introduce in this notebook are:\n",
    "- Linear regression (Supervised learning)\n",
    "- Random forest classifier (Supervised learning)\n",
    "- K-nearest neighbors classifier (Supervised learning)\n",
    "- K-means clustering (Unsupervised learning)\n",
    "- DBSCAN clustering (Unsupervised learning)\n",
    "\n",
    "Feel free to explore other models and techniques as well. The goal of this notebook is to give you a starting point for making predictions with your data.\n",
    "\n",
    "You may notice the steps for fitting the models and making predictions are quite similar. This is because the process of fitting a model and making predictions is generally the same regardless of the model you are using. The main difference is in the type of model you are using and the specific parameters you need to set for that model."
   ],
   "id": "b453fdc67436d277"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "import matplotlib.pyplot as plt"
   ],
   "id": "99a55e151dc7cf18"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the processed data\n",
    "DATA_PATH = \"../data/processed/processed_data.csv\"\n",
    "data = pd.read_csv(DATA_PATH)"
   ],
   "id": "12e5a68e1cd2ab18"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Linear regression\n",
    "# For linear regression, we will use the features \"feature1\" and \"feature2\" to predict the target variable \"target\". You can replace these with the actual feature names from your dataset.\n",
    "regression_data = data[[\"feature1\", \"feature2\", \"target\"]]\n",
    "X = regression_data[[\"feature1\", \"feature2\"]]\n",
    "y = regression_data[\"target\"]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train, test = train_test_split(regression_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the linear regression model and make predictions\n",
    "model = LinearRegression()\n",
    "model.fit(train[[\"feature1\", \"feature2\"]], train[\"target\"])\n",
    "predictions = model.predict(test[[\"feature1\", \"feature2\"]])\n",
    "\n",
    "# Check the accuracy of the model\n",
    "accuracy_score(test[\"target\"], predictions)"
   ],
   "id": "d3522d9208538456"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Random forest classifier\n",
    "# For random forest classifier, we will use the features \"feature1\" and \"feature2\" to predict the target variable \"target\". You can replace these with the actual feature names from your dataset.\n",
    "# To choose all columns except the target column, you can use classification_data.drop(\"target\", axis=1)\n",
    "classification_data = data[[\"feature1\", \"feature2\", \"target\"]]\n",
    "X = classification_data[[\"feature1\", \"feature2\"]]\n",
    "y = classification_data[\"target\"]\n",
    "\n",
    "# Like before, split the data into training and testing sets\n",
    "train, test = train_test_split(regression_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the random forest classifier and make predictions\n",
    "model = RandomForestClassifier()\n",
    "model.fit(train[[\"feature1\", \"feature2\"]], train[\"target\"])\n",
    "predictions = model.predict(test[[\"feature1\", \"feature2\"]])\n",
    "\n",
    "# Check the accuracy of the model\n",
    "accuracy_score(test[\"target\"], predictions)"
   ],
   "id": "9d9abad1b263b7da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# K-nearest neighbors classifier\n",
    "# For K-nearest neighbors classifier, we will use the features \"feature1\" and \"feature2\" to predict the target variable \"target\". You can replace these with the actual feature names from your dataset.\n",
    "# To choose all columns except the target column, you can use classification_data.drop(\"target\", axis=1)\n",
    "classification_data = data[[\"feature1\", \"feature2\", \"target\"]]\n",
    "X = classification_data[[\"feature1\", \"feature2\"]]\n",
    "y = classification_data[\"target\"]\n",
    "\n",
    "# Like before, split the data into training and testing sets\n",
    "train, test = train_test_split(regression_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the K-nearest neighbors classifier and make predictions\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(train[[\"feature1\", \"feature2\"]], train[\"target\"])\n",
    "predictions = model.predict(test[[\"feature1\", \"feature2\"]])\n",
    "\n",
    "# Check the accuracy of the model\n",
    "accuracy_score(test[\"target\"], predictions)"
   ],
   "id": "f08548f58ef43721"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# K-means clustering\n",
    "# For K-means clustering, we will use the features \"feature1\" and \"feature2\" to cluster the data. You can replace these with the actual feature names from your dataset.\n",
    "clustering_data = data[[\"feature1\", \"feature2\"]]\n",
    "\n",
    "# Find the optimal number of clusters using the elbow method\n",
    "for k in range(1, 10):\n",
    "    model = KMeans(n_clusters=k)\n",
    "    model.fit(clustering_data)\n",
    "    plt.plot(k, model.inertia_, \"bo\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.title(\"Elbow Method for Optimal k\")\n",
    "plt.show()\n",
    "\n",
    "# Examine the plot and choose the optimal number of clusters (the point where the inertia starts to decrease significantly). In this example, we will use 3 clusters. You can change this number based on your analysis of the elbow plot.\n",
    "\n",
    "# Fit the K-means clustering model and make predictions\n",
    "model = KMeans(n_clusters=3)\n",
    "model.fit(clustering_data)\n",
    "predictions = model.predict(clustering_data)\n",
    "\n",
    "# Check the accuracy of the model (since this is unsupervised learning, we will not have a target variable to compare against. Instead, we can visualize the clusters to see if they make sense.)\n",
    "plt.scatter(clustering_data[\"feature1\"], clustering_data[\"feature2\"], c=predictions, cmap=\"viridis\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"K-means Clustering\")\n",
    "plt.show()"
   ],
   "id": "35a227e0257a77e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# DBSCAN clustering\n",
    "# For DBSCAN clustering, we will use the features \"feature1\" and \"feature2\" to cluster the data. You can replace these with the actual feature names from your dataset.\n",
    "clustering_data = data[[\"feature1\", \"feature2\"]]\n",
    "\n",
    "# Fit the DBSCAN clustering model and make predictions\n",
    "model = DBSCAN(eps=0.5, min_samples=5)\n",
    "model.fit(clustering_data)\n",
    "predictions = model.labels_\n",
    "\n",
    "# Check the accuracy of the model (since this is unsupervised learning, we will not have a target variable to compare against. Instead, we can visualize the clusters to see if they make sense.)\n",
    "plt.scatter(clustering_data[\"feature1\"], clustering_data[\"feature2\"], c=predictions, cmap=\"viridis\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"DBSCAN Clustering\")"
   ],
   "id": "ef17a15729e947e8"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
